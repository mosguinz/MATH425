\documentclass[12pt]{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage{fancyhdr,latexsym,amssymb,amsmath,graphicx}
\usepackage{algorithm}
\usepackage{cancel}
\usepackage[noend]{algpseudocode}
\usepackage[pdftex]{hyperref}
\pagestyle{fancy}
\usepackage[parfill]{parskip} % Do not indent between empty lines
\usepackage{mathtools}
\usepackage{nicematrix}
\usepackage{xfrac}
\usepackage{bm}
\usepackage{esvect}

\newcommand{\veq}{\mathrel{\rotatebox{90}{$=$}}}
\newcommand{\vneq}{\mathrel{\rotatebox{90}{$\neq$}}}
\newcommand{\vect}[1]{\vv{\mathbf{#1}}}
\newcommand{\code}[1]{\texttt{#1}}

\lhead{MATH 425}
\chead{Homework 2}
\rhead{Sitthisarnwattanachai}

\title{510 HW1}
\author{mosguinz}
\date{February 2024}

\begin{document}

\section*{Question 1}

The functions are defined in the \code{math425hw2.m} file, under the section \code{\%\% Question 1}.

\subsection*{Part A}

See \code{math425hw2.m}.

\subsection*{Part B}

Using \code{myPartialPivot}, \code{myRank} simply counts the number of non-zero pivots in the upper triangular matrix returned from \code{myPartialPivot}.

Note that, even with partial pivoting, the computation still suffer from the precision issue of floating-point arithmetic. For this reason, in the implementation for \code{myRank}, I have rounded pivot entries to the tenth decimal place to determine whether they are really zeroes.

For example, consider a matrix
$$
A=\begin{pmatrix}
    1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9
\end{pmatrix}.
$$

For the first pivot, we would row swap $R_3$ and $R_1$ and perform the row eliminations as follows.

$$
\begin{pmatrix}
     7 & 8 & 9 \\
     4 & 5 & 6 \\
     1 & 2 & 3
\end{pmatrix}
\xrightarrow[]{-\frac{4}{7}R_1 + R_2 \to R_2}
\begin{pmatrix}
     7 & 8 & 9 \\
     0 & \sfrac{3}{7} & \sfrac{6}{7} \\
     1 & 2 & 3
\end{pmatrix}
\xrightarrow[]{-\frac{1}{7}R_1 + R_3 \to R_3}
\begin{pmatrix}
     7 & 8 & 9 \\
     0 & \sfrac{3}{7} & \sfrac{6}{7} \\
     0 & \sfrac{6}{7} & \sfrac{12}{7}
\end{pmatrix}
$$

For the second pivot, we would row swap $R_3$ and $R_2$, and perform the row as follows. This also happens to eliminate the third pivot. Thus, the rank is 2.

$$
\begin{pmatrix}
     7 & 8 & 9 \\
     0 & \sfrac{6}{7} & \sfrac{12}{7} \\
     0 & \sfrac{3}{7} & \sfrac{6}{7}
\end{pmatrix}
\xrightarrow[]{-\frac{1}{2}R_2 + R_3 \to R_3}
\begin{pmatrix}
     7 & 8 & 9 \\
     0 & \sfrac{6}{7} & \sfrac{12}{7} \\
     0 & 0 & 0
\end{pmatrix}
$$

The above should be our upper triangular matrix, $U$. However, due floating point error, the result is not quiet zero. In MATLAB, when we display $U$ using \code{format rational}, it displays:

\begin{verbatim}
       7              8              9       
       0              6/7           12/7     
       0              *              *   
\end{verbatim}

where the asterisks denote that the denominator is too large. By using \code{format long} we can see that the result is pretty much zero:

\begin{verbatim}
    
   7.000000000000000   8.000000000000000   9.000000000000000
                   0   0.857142857142857   1.714285714285714
                   0   0.000000000000000   0.000000000000000
\end{verbatim}

Using \code{format longG} we can see that the stored values aren't quiet zero. Hence, I have rounded the digits to ten decimal places.

\begin{verbatim}
       7                         8                         9
       0         0.857142857142857          1.71428571428571
       0      5.55111512312578e-17      1.11022302462516e-16
\end{verbatim}

\subsection*{Part C}

Yes, the function virtually always returns 3. Since the question did not specify the bounds for the values from which a matrix could be generated, I simply used \code{rand(5, 3)} and \code{rand(3, 5)} to generate a $5\times3$ and $3\times5$ matrix, respectively. As such, it is virtually impossible for two or more rows to be identical or be a multiple of each other.

Furthermore, the floating-point precision issue discussed in part (b) would almost certainly ensure that the result appears to be full rank --- especially when the values in the matrix are floating-point numbers to begin with --- as values that should be zero may instead be interpreted as non-zero.

\section*{Question 2}

The relevant snippets are defined in the \code{math425hw2.m} file, under the section \code{\%\% Question 2}.

\subsection*{Part A}

Let $A = \begin{pmatrix}
    4 & 1 & 1 & 1 \\
    1 & 4 & 1 & 1 \\
    1 & 1 & 4 & 1 \\
    1 & 1 & 1 & 4 \\
\end{pmatrix}
$. $A$ is clearly not diagonal and the sum of the off-diagonal entries in each column is 3, and all the diagonal entries (pivots) are 4. Since the diagonal entries are greater than the sum of the absolute values of the off-diagonal entries in each column, $A$ is a $4\times4$ strictly column diagonally dominant matrix.

\subsection*{Part B}

In Gaussian elimination with partial pivoting, at each step, the algorithm searches for the largest absolute value in the current column below the diagonal and swaps rows if necessary to ensure that this value is used as the pivot. Row interchanges occur if an off-diagonal element is larger than the diagonal element.

If a matrix $A$ is a strictly column diagonally dominant matrix, then the absolute value of the entry $a_{jj}$ for any column $j$ must be strictly larger than the (sum of the) absolute values of all other entries in that column. Thus, no row interchanges will occur.

\subsection*{Part C}

In \code{myPartialPivot}, a counter was added to keep track of the number of row interchanges. Sure enough, using the matrix $A$ in part (a) yields the following output:

\begin{verbatim}
The number of row interchanges is 0.
ans =

       4              1              1              1       
       0             15/4            3/4            3/4     
       0              0             18/5            3/5     
       0              0              0              7/2     
\end{verbatim}

To verify that the function works correctly when row interchanges are required, I tested it using the matrix from part 1(b), which we demonstrated needs two row interchanges. The output for that matrix is as follows:

\begin{verbatim}
The number of row interchanges is 2.
ans =

       7              8              9       
       0              6/7           12/7     
       0              *              *     
\end{verbatim}

Again, the asterisks in the second output are due to floating-point precision errors, as discussed in that question.

\section*{Question 3}

A symmetric matrix $A$ is a matrix such that $A=A^\top$. That is, if $A$ is square and all of its entries satisfy $a_{ij}=a_{ji}$ for all indices $i$ and $j$. If we are able to reduce $A$ through Gaussian elimination such that the resulting matrix $D$ is symmetric, it \textit{necessarily} follows that the all the off-diagonal entries in the matrix must be zero. That is,

$$
A = \begin{pNiceArray}{ccccc}
    a_{1,1} & a_{1,2} & \Cdots & & a_{1,n} \\
    a_{1,2} & a_{2,2} & \Cdots & & a_{2,n} \\
    \Vdots  & \Vdots  &        &            \\
            &         &        & \Ddots & \Vdots \\
    a_{1,n} & a_{n,2} & \Cdots & & a_{n,n}
\end{pNiceArray}
\quad
\leadsto
\quad
D = \begin{pNiceArray}{ccccc}
    d_{1,1} & 0       & \Cdots &        & 0         \\
    0       & d_{2,2} &        &        &           \\
    \Vdots  & 0       &        &        & \Vdots         \\
            & \Vdots  &        & \Ddots & 0   \\
    0       & 0       & \Cdots & 0      & d_{n,n}     
\end{pNiceArray}.
$$

If $A$ is symmetric and regular, then it may be factored as $A=LDL^\top$, where $L$ is a lower unitriangular matrix and $D$ is a diagonal matrix with nonzero diagonal entries (Olver and Shakiban 45).

By Theorem 1.29 from the text, the $LU$ factorization of $A$ may be written as $A=LDV$, where $L$ is a lower unitriangular matrix, $D$ is a diagonal matrix with nonzero diagonal entries, and $V$ is an upper unitriangular matrix. Here, $U=DV$ where $D$ contains the corresponding pivots of $U$ on its diagonal, with all its other off-diagonal entries as zero, and $V$ be derived from $U$, where each row is divided by the value of its pivot (ibid. 41).

The factorization of $A=LDV$ shows that if $A$ is symmetric, then taking the transpose of $A$ yields
$$
A = A^\top = (LDV)^\top = V^\top D^\top L^\top.
$$

To find such a diagonal matrix $D$, we first obtain the upper triangular matrix $U$ by performing Gaussian elimination as usual. Then, where $U=DV$ as defined above, we have that
\begin{align*}
    U &= DV
    \\
    &= \begin{pNiceArray}{ccccc}
        u_{1,1} & u_{1,2} & \Cdots & & u_{1,n} \\
        0 & u_{2,2} & & & \Vdots \\
        \Vdots & & & \\
        & & & \Ddots\\
        0 & 0 & \Cdots & & u_{n,n}
    \end{pNiceArray}
    \\
    &= \begin{pNiceArray}{ccccc}
        d_{1,1} & 0 & \Cdots & & 0 \\
        0 & d_{2,2} & & & \Vdots \\
        \Vdots & & & \\
        & & & \Ddots\\
        0 & 0 & \Cdots & & d_{n,n}
    \end{pNiceArray}
    \begin{pNiceArray}{ccccc}
        1 & \frac{u_{1,2}}{u_{1,1}} & \Cdots & & \frac{u_{1,n}}{u_{1,1}} \\
        0 & 1 & \frac{u_{2,3}}{u_{2,2}} & \Cdots & \frac{u_{2,n}}{u_{2,2}} \\
        \Vdots & & & & \Vdots \\
        & & & \Ddots & \frac{u_{n-1,n}}{u_{n-1,n-1}}\\
        0 & 0 & \Cdots & & 1
    \end{pNiceArray}
\end{align*}

where $D$ is the diagonal matrix. Again, since $A$ is symmetric $D=D^\top$ must be automatically be symmetric. 

\end{document}
