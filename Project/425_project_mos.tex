\documentclass[12pt]{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage{fancyhdr,latexsym,amssymb,amsmath,graphicx}
\usepackage{algorithm}
\usepackage{cancel}
\usepackage[noend]{algpseudocode}
\usepackage[pdftex]{hyperref}
\pagestyle{fancy}
\usepackage[parfill]{parskip} % Do not indent between empty lines
\usepackage{mathtools}
\usepackage{nicematrix}
\usepackage{xfrac}
\usepackage{bm}
\usepackage{esvect}
\usepackage{svg}


\setcounter{MaxMatrixCols}{20} % fatass matrix

% For the \set notation
\usepackage{xparse} 
\DeclarePairedDelimiterX{\set}[1]{\{}{\}}{\setargs{#1}}
\NewDocumentCommand{\setargs}{>{\SplitArgument{1}{;}}m}
{\setargsaux#1}
\NewDocumentCommand{\setargsaux}{mm}
{\IfNoValueTF{#2}{#1} {#1\,\delimsize|\,\mathopen{}#2}}%{#1\:;\:#2}

% Some common symbols
\newcommand{\veq}{\mathrel{\rotatebox{90}{$=$}}}
\newcommand{\vneq}{\mathrel{\rotatebox{90}{$\neq$}}}
\newcommand{\vect}[1]{\vv{\mathbf{#1}}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\rank}{\operatorname{rank}}

% \lhead{MATH 425}
\lhead{Computation of the Camera Matrix}
\rhead{Sitthisarnwattanachai}

\title{Computation of the Camera Matrix}
\author{Kullathon ``Mos" Sitthisarnwattanachai}
\date{December 22, 2024}

\begin{document}

\maketitle

\section{Introduction}

In computer vision, the camera matrix is a $3 \times 4$ matrix that encapsulates the mapping performed by a pinhole camera, projecting 3D points in the real world onto 2D points in an image. This ``mini"-project focuses on understanding and implementing the simplest camera model, the basic pinhole camera.

We will begin by describing the projection of points in 3D space onto a 2D plane using a series of matrices that we will define. The majority of this report is based on the explanations provided in \textit{Multiple View Geometry in Computer Vision} by Richard Hartley and Andrew Zisserman.

\section{The pinhole camera model}

\begin{figure}
    \centering 
    \includesvg[height = 250pt]{fig1.svg}
    \caption{Pinhole camera geometry. $\mathbf{C}$ is the camera centre and $\mathbf{p}$ the principal point. The camera center is here placed at the coordinate origin.}
    \label{fig:enter-label}
\end{figure}

The pinhole camera model is a simplified representation of how a camera projects three-dimensional (3D) points in space onto a two-dimensional (2D) image plane. Under this model, we aim to map a 3D point, denoted by 
$\mathbf{X} = \begin{pmatrix} x & y & z \end{pmatrix}^\top$, 
to its corresponding point on the image plane. This point is the intersection of the line passing through $\mathbf{X}$ and the center of projection (the camera's optical center) with the image plane.

The geometry of the setup defines the center of projection at the origin of the camera coordinate system, with the image plane located at a fixed distance $f$ (the focal length) from the origin. The projection follows the principle of similar triangles.


The equations governing this projection are given by:
$$
u = \frac{f x}{z}, \quad v = \frac{f y}{z}.
$$

Now, note that these perspective projections are nonlinear due to the division by $z$. To simplify our operations, we want to transform our system using a linear model. In our case, we can accomplish this by using homogeneous coordinates.

\subsection{Linearizing and using homogeneous coordinates}

In the homogeneous form, the coordinates of our point $\mathbf{X}$ are expressed as $\begin{pmatrix} x & y & z & 1 \end{pmatrix}^\top$, and the corresponding point on the image plane becomes $\begin{pmatrix} u & v & z \end{pmatrix}^\top$. Then, we can write our projection in matrix form as:
$$
\begin{pmatrix} u \\ v \\ z \end{pmatrix} = 
\begin{pmatrix}
    f & 0 & 0 & 0 \\
    0 & f & 0 & 0 \\
    0 & 0 & 1 & 0
\end{pmatrix}
\begin{pmatrix} x \\ y \\ z \\ 1 \end{pmatrix}.
$$

This homogeneous representation serves as the foundation for deriving the complete camera matrix, which incorporates additional parameters to account for intrinsic and extrinsic camera properties.

\subsection{Intrinsic matrix}

In real-world cameras, the optical center does not necessarily coincide with the center of the image plane. Instead, the projection may be offset by a certain amount. This offset is characterized by the principal point, denoted by $p_x$ and $p_y$ for the horizontal and vertical displacements, respectively.

Thus, the projection equations are modified to account for this offset:
$$
u = \frac{f x}{z} + p_x, \quad v = \frac{f y}{z} + p_y.
$$

In the homogeneous representation, this is expressed as:
$$
\begin{pmatrix} u \\ v \\ z \end{pmatrix} = 
\begin{pmatrix}
f & 0 & p_x & 0 \\
0 & f & p_y & 0 \\
0 & 0 & 1 & 0
\end{pmatrix}
\begin{pmatrix} x \\ y \\ z \\ 1 \end{pmatrix}.
$$

Here, the $3 \times 3$ submatrix
$$
\mathbf{K} = \begin{pmatrix}
f & 0 & p_x \\
0 & f & p_y \\
0 & 0 & 1
\end{pmatrix}
$$
is known as the \textit{camera calibration matrix}, which encapsulates the \textit{intrinsic parameters} of the camera, such as the focal length $f$ and the principal point offsets, $p_x$ and $p_y$.

The entire matrix is referred to as the \textit{intrinsic matrix}. Now, our 2D coordinate  $\mathbf{x}$ can be expressed as
$$
\mathbf{x} = \mathbf{K}\begin{bmatrix}
    I & | & \mathbf{0}
\end{bmatrix}\mathbf{x}_\text{cam}
$$
where $\mathbf{x}_\text{cam}=\begin{pmatrix}
    x & y & z & 1
\end{pmatrix}^\top$ is our camera at the origin of a Euclidean coordinate system.

\subsection{Extrinsic matrix}

In addition to the intrinsic properties of a camera, we must account for its position and orientation in the world. These are described by the \textit{extrinsic parameters}, which define the relationship between the \textit{camera coordinate frame} and the \textit{world coordinate frame}. 

The transformation between the world coordinate frame and the camera coordinate frame involves two components:
\begin{itemize}
    \item A $3 \times 3$ rotation matrix $\mathbf{R}$, which describes the orientation of the camera with respect to the world coordinate frame. Note that this matrix is orthogonal.
    \item A $3 \times 1$ translation vector $\mathbf{t}$, which specifies the position of the camera's optical center in the world coordinate frame.
\end{itemize}

The matrix $\mathbf{R}$ has three rows, each representing a basis vector of the camera coordinate frame expressed in the world coordinate frame:
\begin{itemize}
    \item The first row of $\mathbf{R}$ represents the direction of the camera's $x$-axis in the world coordinate frame.
    \item The second row of $\mathbf{R}$ represents the direction of the camera's $y$-axis in the world coordinate frame.
    \item The third row of $\mathbf{R}$ represents the direction of the camera's optical axis (or $z$-axis) in the world coordinate frame.
\end{itemize}

We again express the system in terms of a homogeneous coordinate to transform a point $\mathbf{X}_{\text{w}}$ in the world coordinate frame to the camera coordinate frame, we apply the rotation and translation as follows:
$$
\mathbf{x}_\text{cam} = 
\mathbf{R} \mathbf{X}_{\text{w}} + \mathbf{t} =
\begin{pmatrix} 
\mathbf{R} & \mathbf{t} \\
0 & 1
\end{pmatrix}
\begin{pmatrix} x_{\text{w}} \\ y_{\text{w}} \\ z_{\text{w}} \\ 1 \end{pmatrix}.
$$

Here, the $4\times 4$ matrix $
\begin{pmatrix} 
\mathbf{R} & \mathbf{t} \\
0 & 1
\end{pmatrix}
$
is known as the \textit{extrinsic matrix}, combining rotation and translation into a single transformation.

\section{The camera matrix}

The camera matrix combines the intrinsic and extrinsic transformations to map a 3D point in the world coordinate frame directly onto the 2D image plane.

The intrinsic matrix $\mathbf{K}$ maps a point from the camera coordinate system to the 2D image plane. The extrinsic matrix $\begin{bmatrix} \mathbf{R} & | & \mathbf{t} \end{bmatrix}$ transforms a point from the world coordinate frame to the camera coordinate frame, where the rotation matrix $\mathbf{R}$ encodes the orientation of the camera and the translation vector $\mathbf{t}$ specifies its position.

Together, the intrinsic and extrinsic matrices form our $3\times 4$ camera matrix, $\mathbf{P}$:
\begin{align*}
    \mathbf{P} &= \mathbf{K} \begin{bmatrix} \mathbf{R} & | & \mathbf{t} \end{bmatrix} \\
    &= \begin{pmatrix}
        f & 0 & p_x & 0 \\
        0 & f & p_y & 0 \\
        0 & 0 & 1 & 0
    \end{pmatrix}
    \begin{pmatrix}
        r_{11} & r_{12} & r_{13} & t_x \\
        r_{21} & r_{22} & r_{23} & t_y \\
        r_{31} & r_{32} & r_{33} & t_z \\
        0 & 0 & 0 & 1
    \end{pmatrix}.
\end{align*}

The transformation can now be expressed compactly as:
$$
\mathbf{x}_{\text{image}} = \mathbf{P} \mathbf{X}_{\text{w}},
$$
where $\mathbf{X}_{\text{w}}$ is a 3D point in homogeneous world coordinates, and $\mathbf{x}_{\text{image}}$ is its corresponding 2D point in homogeneous image coordinates.

Now that we have a complete overview of what the camera matrix $ \mathbf{P} $ is composed of, we can compute an initial estimate for $ \mathbf{P} $ using the Gold Standard algorithm outlined in the textbook (Hartley and Zisserman 181). The remainder of this report will be dedicated to computing each of the following steps.

\subsection{The Gold Standard algorithm to estimate the camera matrix}
\label{sec:GoldStandard}

\paragraph{(1) Normalization}
We first normalize both the image points and the 3D space points:
\begin{itemize}
    \item Apply a similarity transformation $ \mathbf{T} $ to normalize the image points. This involves translating and scaling the points so that their centroid is at the origin and their average distance from the origin is $\sqrt{2}$.
    \item Similarly, apply a similarity transformation $ \mathbf{U} $ to normalize the 3D space points so that their centroid is at the origin, and their average distance from the origin is $\sqrt{3}$.
\end{itemize}

\paragraph{(2) Direct linear transformation (DLT)}
The next step is to form a linear system of equations:
\begin{itemize}
    \item For each correspondence $ \mathbf{X}_i \leftrightarrow \mathbf{x}_i $, use the normalized coordinates $ \tilde{\mathbf{X}}_i $ and $ \tilde{\mathbf{x}}_i $ to generate two equations based on the projection model $ \tilde{\mathbf{x}}_i = \mathbf{P} \tilde{\mathbf{X}}_i $.
    \item Stack all the equations into a $ 2n \times 12 $ matrix $ \mathbf{A} $, where $ n $ is the number of correspondences.
    \item Write the entries of $ \mathbf{P} $ as a 12-dimensional vector $ \mathbf{p} $. The system $ \mathbf{A} \mathbf{p} = 0 $ is then solved subject to the constraint $ \| \mathbf{p} \| = 1 $.
    \item The solution is obtained from the unit singular vector of $ \mathbf{A} $ corresponding to its smallest singular value.
\end{itemize}

\paragraph{(3) Denormalization}
Finally, denormalize the estimated $ \mathbf{P} $ to obtain the camera matrix in the original coordinate system:
$$
\mathbf{P} = \mathbf{T}^{-1} \tilde{\mathbf{P}} \mathbf{U}.
$$

Here, $ \mathbf{T} $ and $ \mathbf{U} $ are the inverse similarity transformations used for normalization, and $ \tilde{\mathbf{P}} $ is the intermediate result obtained from the DLT step.

\section{Normalization}

Normalization is a crucial preprocessing step in the Gold Standard algorithm, designed to improve numerical stability when estimating the camera matrix. By scaling and centering the points appropriately, we mitigate the effects of numerical inaccuracies in the subsequent computations.

\textit{Data normalization is an essential step in the DLT algorithm and must not be considered optional} (ibid. 108).

\subsection{Normalizing image points}
Given a set of image points $ \mathbf{x}_i = \begin{pmatrix} u_i & v_i \end{pmatrix}^\top $ in homogeneous coordinates, the goal is to apply a similarity transformation $ \mathbf{T} $ such that:
\begin{itemize}
    \item The centroid of the points is shifted to the origin.
    \item The average distance of the points from the origin is $\sqrt{2}$.
\end{itemize}

To achieve this, the transformation matrix $ \mathbf{T} $ is defined as:
$$
\mathbf{T} = 
\begin{pmatrix} 
s & 0 & -s \bar{u} \\
0 & s & -s \bar{v} \\
0 & 0 & 1 
\end{pmatrix},
$$
where:
\begin{itemize}
    \item $ \bar{u}, \bar{v} $ are the centroid coordinates of the points, computed as:
    $$
    \bar{u} = \frac{1}{n} \sum_{i=1}^n u_i, \quad \bar{v} = \frac{1}{n} \sum_{i=1}^n v_i.
    $$
    \item $ s $ is the scaling factor, defined to ensure the average distance from the origin is $\sqrt{2}$:
    $$
    s = \frac{\sqrt{2}}{\displaystyle\frac{1}{n} \sum_{i=1}^n \sqrt{(u_i - \bar{u})^2 + (v_i - \bar{v})^2}}.
    $$
\end{itemize}

\subsection{Normalizing 3D space points}
Similarly, for a set of 3D space points $ \mathbf{X}_i = \begin{pmatrix} x_i & y_i & z_i \end{pmatrix}^\top $ in homogeneous coordinates, we apply a similarity transformation $ \mathbf{U} $ such that:
\begin{itemize}
    \item The centroid of the points is shifted to the origin.
    \item The average distance of the points from the origin is $\sqrt{3}$.
\end{itemize}

The transformation matrix $ \mathbf{U} $ is defined as:
$$
\mathbf{U} = 
\begin{pmatrix} 
s_x & 0 & 0 & -s_x \bar{x} \\
0 & s_y & 0 & -s_y \bar{y} \\
0 & 0 & s_z & -s_z \bar{z} \\
0 & 0 & 0 & 1
\end{pmatrix},
$$
where:
\begin{itemize}
    \item $ \bar{x}, \bar{y}, \bar{z} $ are the centroid coordinates of the 3D points, computed as:
    $$
    \bar{x} = \frac{1}{n} \sum_{i=1}^n x_i, \quad \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i, \quad \bar{z} = \frac{1}{n} \sum_{i=1}^n z_i.
    $$
    \item $ s_x, s_y, s_z $ are the scaling factors for each axis, defined to ensure the average distance from the origin is $\sqrt{3}$:
    $$
    s_x = s_y = s_z = \frac{\sqrt{3}}{\displaystyle\frac{1}{n} \sum_{i=1}^n \sqrt{(x_i - \bar{x})^2 + (y_i - \bar{y})^2 + (z_i - \bar{z})^2}}.
    $$
\end{itemize}

After normalization, we have that:
\begin{itemize}
    \item the image points $ \mathbf{x}_i $ are transformed into normalized coordinates $ \tilde{\mathbf{x}}_i = \mathbf{T} \mathbf{x}_i $; and
    \item the 3D space points $ \mathbf{X}_i $ are transformed into normalized coordinates $ \tilde{\mathbf{X}}_i = \mathbf{U} \mathbf{X}_i $.
\end{itemize}
The transformations $ \mathbf{T} $ and $ \mathbf{U} $ ensure that the numerical conditioning of the problem is improved, making the DLT step more stable and accurate.

\section{Performing the direct linear transformation (DLT)}

\subsection{Setting up the linear system}

Once the points have been normalized using the transformations $ \mathbf{T} $ and $ \mathbf{U} $, we estimate the camera matrix $ \mathbf{P} $ in the normalized coordinate system. 

For each correspondence $ \tilde{\mathbf{x}}_i \leftrightarrow \tilde{\mathbf{X}}_i $, the relationship between the normalized 2D image point $ \tilde{\mathbf{x}}_i $ and the normalized 3D space point $ \tilde{\mathbf{X}}_i $ is given by:
$$
\tilde{\mathbf{x}}_i = \mathbf{P} \tilde{\mathbf{X}}_i,
$$
which we write in matrix form as
$$
\begin{pmatrix} 
    \tilde{u}_i \\ \tilde{v}_i \\ 1
\end{pmatrix} = 
\begin{pmatrix} 
p_{11} & p_{12} & p_{13} & p_{14} \\
p_{21} & p_{22} & p_{23} & p_{24} \\
p_{31} & p_{32} & p_{33} & p_{34} 
\end{pmatrix}
\begin{pmatrix} 
\tilde{X}_i \\ \tilde{Y}_i \\ \tilde{Z}_i \\ 1
\end{pmatrix}.
$$

For each corresponding point $\tilde{\mathbf{x}}_i$, we can expand the matrix to give us a pair of equations:
\begin{align*}
\tilde{u}_i &= \frac{\mathbf{p}_1^\top \tilde{\mathbf{X}}_i}{\mathbf{p}_3^\top \tilde{\mathbf{X}}_i}
= \frac{p_{11} \tilde{X}_i + p_{12} \tilde{Y}_i + p_{13} \tilde{Z}_i + p_{14}}{p_{31} \tilde{X}_i + p_{32} \tilde{Y}_i + p_{33} \tilde{Z}_i + p_{34}},\quad\text{and}
\\
\quad \tilde{v}_i &= \frac{\mathbf{p}_2^\top \tilde{\mathbf{X}}_i}{\mathbf{p}_3^\top \tilde{\mathbf{X}}_i}
= \frac{p_{21} \tilde{X}_i + p_{22} \tilde{Y}_i + p_{23} \tilde{Z}_i + p_{24}}{p_{31} \tilde{X}_i + p_{32} \tilde{Y}_i + p_{33} \tilde{Z}_i + p_{34}}.
\end{align*}

Now, we can rewrite the equations again in matrix form. For $n$ correspondences, we stack the equations into a $2n \times 12$ matrix $\mathbf{A}$, containing all the known elements. Then, writing our camera matrix $\mathbf{P}$ as a 12-dimensional vector $\mathbf{p}$, we have a homogeneous system $\mathbf{A}\mathbf{p}=\mathbf{0}$:

$$
\begin{pmatrix}
\tilde{X}_1 & \tilde{Y}_1 & \tilde{Z}_1 & 1 & 0 & 0 & 0 & 0 & -\tilde{u}_1 \tilde{X}_1 & -\tilde{u}_1 \tilde{Y}_1 & -\tilde{u}_1 \tilde{Z}_1 & -\tilde{u}_1 \\
0 & 0 & 0 & 0 & \tilde{X}_1 & \tilde{Y}_1 & \tilde{Z}_1 & 1 & -\tilde{v}_1 \tilde{X}_1 & -\tilde{v}_1 \tilde{Y}_1 & -\tilde{v}_1 \tilde{Z}_1 & -\tilde{v}_1 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
\tilde{X}_i & \tilde{Y}_i & \tilde{Z}_i & 1 & 0 & 0 & 0 & 0 & -\tilde{u}_i \tilde{X}_i & -\tilde{u}_i \tilde{Y}_i & -\tilde{u}_i \tilde{Z}_i & -\tilde{u}_i \\
0 & 0 & 0 & 0 & \tilde{X}_i & \tilde{Y}_i & \tilde{Z}_i & 1 & -\tilde{v}_i \tilde{X}_i & -\tilde{v}_i \tilde{Y}_i & -\tilde{v}_i \tilde{Z}_i & -\tilde{v}_i \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
\tilde{X}_n & \tilde{Y}_n & \tilde{Z}_n & 1 & 0 & 0 & 0 & 0 & -\tilde{u}_n \tilde{X}_n & -\tilde{u}_n \tilde{Y}_n & -\tilde{u}_n \tilde{Z}_n & -\tilde{u}_n \\
0 & 0 & 0 & 0 & \tilde{X}_n & \tilde{Y}_n & \tilde{Z}_n & 1 & -\tilde{v}_n \tilde{X}_n & -\tilde{v}_n \tilde{Y}_n & -\tilde{v}_n \tilde{Z}_n & -\tilde{v}_n
\end{pmatrix}
\begin{pmatrix} 
p_{11} \\ p_{12} \\ p_{13} \\ p_{14} \\
p_{21} \\ p_{22} \\ p_{23} \\ p_{24} \\
p_{31} \\ p_{32} \\ p_{33} \\ p_{34} 
\end{pmatrix}
= 0.
$$

\subsection{Solving the system using singular value decomposition}

To solve $ \mathbf{A} \mathbf{p} = 0 $, subject to $ \| \mathbf{p} \| = 1 $, we use can simply utilize the singular value decomposition (SVD), $\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top$.

Where $\mathbf{\Sigma}=\operatorname{diag}(\sigma_1, \sigma_2, \ldots, \sigma_n)$ contains the singular values such that $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_n > 0$, we denote the solution $ \tilde{\mathbf{p}} $ to be the unit singular vector corresponding to the smallest singular value in $ \mathbf{\Sigma} $, i.e., the last column of $ \mathbf{V}$.

Reshaping our solution $\tilde{\mathbf{p}}$ back into a $3 \times 4$ matrix gives the normalized estimate of $\mathbf{P}$, which we shall denote as $\tilde{\mathbf{P}}$.

\section{Denormalization}

Finally, we then denormalize the matrix $\tilde{\mathbf{P}}$ that we just computed to obtain our camera matrix $ \mathbf{P} $ in the original coordinate system:
$$
\mathbf{P} = \mathbf{T}^{-1} \tilde{\mathbf{P}} \mathbf{U}.
$$

\section{Conclusion}

The camera matrix serves as a fundamental tool in computer vision, enabling us to project 3D world points onto a 2D image plane. By combining the intrinsic parameters, which map camera coordinates to image coordinates, and the extrinsic parameters, which map world coordinates to camera coordinates, the camera matrix encapsulates the entire process of image formation in the pinhole camera model. This makes it invaluable for tasks such as 3D reconstruction, camera calibration, and image-based localization.

\subsection{Limitations}

To maintain the scope of this project and keep it focused as a ``mini" project, certain variables and complexities have been deliberately omitted from our model.

\paragraph{Intrinsic parameters} When defining the intrinsic matrix, we assumed that the camera pixels are square. However, in many real-world applications, pixels may be rectangular. This would require differentiating between focal lengths in the \( x \)- and \( y \)-directions, introducing \( f_x \) and \( f_y \) into the model. Moreover, the pinhole camera model does not account for lens distortions, such as radial or tangential distortions, which can significantly impact the accuracy of the projection in practical scenarios.

\paragraph{Number of correspondences} Estimating the camera matrix involves solving for 12 unknowns, which requires at least six point correspondences between 3D world points and their 2D image projections. Accurate estimation depends on these correspondences being precise and well-distributed across the scene. Insufficient points or poorly distributed correspondences can lead to inaccurate results. Additionally, if all points lie on a single plane, a degenerate configuration may arise, resulting in an ambiguous or unreliable camera matrix \( \mathbf{P} \).

\paragraph{Error minimization} This project omits the error minimization step typically included in the Gold Standard algorithm. Minimizing geometric errors, such as reprojection error, requires iterative optimization techniques like Levenberg--Marquardt. While this step improves the accuracy of the camera matrix, it falls outside the scope of this simplified approach.

\subsection{Closing remark on the use of the camera matrix}

Typically, when calculating the camera matrix \( \mathbf{P} \), the focus is not on \( \mathbf{P} \) itself but on the information it encapsulates: the intrinsic parameters, the extrinsic parameters (rotation matrix), and the position of the camera in the world.

The composition of the camera matrix that we detailed at the start can be exploited to reconstruct these matrices. Given that $\mathbf{K}$ is an upper-right triangular matrix and $\mathbf{R}$ is orthonormal, we can decouple $\mathbf{K}$ and $\mathbf{R}$ from the $3\times 3$ submatrix, using $QR$ factorization.
$$
\begin{pmatrix}
    p_{11} & p_{12} & p_{13} \\
    p_{21} & p_{22} & p_{23} \\
    p_{31} & p_{32} & p_{33} \\
\end{pmatrix} = 
\begin{pmatrix}
    f & 0 & p_x & 0 \\
    0 & f & p_y & 0 \\
    0 & 0 & 1 & 0
\end{pmatrix}
\begin{pmatrix}
    r_{11} & r_{12} & r_{13} \\
    r_{21} & r_{22} & r_{23} \\
    r_{31} & r_{32} & r_{33} \\
\end{pmatrix}
= \mathbf{KR}
$$

Having found $\mathbf{K}$, we can then recover the translation vector $\mathbf{t}$ using the last column of $\mathbf{P}$.

$$
\begin{pmatrix}
    p_{14} \\ p_{24} \\ p_{34} \\
\end{pmatrix} = 
\begin{pmatrix}
    f & 0 & p_x & 0 \\
    0 & f & p_y & 0 \\
    0 & 0 & 1 & 0
\end{pmatrix}
\begin{pmatrix}
    t_x \\ t_y \\ t_z
\end{pmatrix}
= \mathbf{Kt}
\quad\implies\quad
\mathbf{t} = \mathbf{K}^{-1}
\begin{pmatrix}
    p_{14} \\ p_{24} \\ p_{34} \\
\end{pmatrix}
$$

By decomposing \( \mathbf{P} \), we can extract the calibration matrix \( \mathbf{K} \), which contains intrinsic parameters such as the focal length and principal point offset, as well as the rotation matrix \( \mathbf{R} \) and the translation vector \( \mathbf{t} \), which describe the cameraâ€™s orientation and position in the world coordinate frame.

\subsection{MATLAB implementation}

The MATLAB implementation of the Gold Standard algorithm is fairly straightforward --- and much less complex than the derivation above. The  \code{math425project.m} file contains the functions that perform the above algorithm:

\begin{itemize}
    \item \verb|normalize_points_2d(points)| and \verb|normalize_points_3d(points)| to normalize the 2D image points and 3D space points, respectively. It returns the normalized, homogeneous coordinates and the respective transformation matrices.
    \item \verb|dlt(image_points, world_points)| to apply the direct linear transformation. It expects the arguments \verb|world_points| and \verb|image_points| to be the \textit{normalized, homogeneous} 2D image and space points and returns the normalized estimate of the camera matrix, denoted $\tilde{\mathbf{P}}$ above.
    Note that the function does not actually enforce that the inputs be normalized. One could apply DLT without normalization. However, as the book mention, doing so is highly discouraged.
    \item \verb|compute_camera_matrix(image_points, world_points)| to execute the entire Gold Standard algorithm, as outlined in \autoref{sec:GoldStandard}. Here, it expects the arguments to be the inhomogeneous sets of 2D image and space points. At the very minimum, it checks that the provided arguments contains at least six pairs of coordinates.
\end{itemize}

It is implemented in such a way that the end-user can simply call \verb|compute_camera_matrix| with their known world-to-image point correspondences. The end of the contains an example on how one could utilize it.

\section{References}

\begin{enumerate}
    \item R. Hartley, A. Zisserman (2004) \textit{Multiple View Geometry in Computer Vision}, 2nd edition. ISBN 978-0521540513.
    \item S. Nayar (2021) ``Linear Camera Model," \textit{First Principles of Computer Vision}. \url{https://fpcv.cs.columbia.edu/}.
    \item S. Nayar (2021) ``Camera Calibration," \textit{First Principles of Computer Vision}. \url{https://fpcv.cs.columbia.edu/}.
    \item S. Nayar (2021) ``Intrinsic and Extinsic Matrices," \textit{First Principles of Computer Vision}. \url{https://fpcv.cs.columbia.edu/}.
    \item C. Stachniss (2021) ``Camera Calibration: Direct Linear Transform," \textit{5 Minutes with Cyrill}. \url{https://www.youtube.com/watch?v=Fdwa0UEJ_F8}.
\end{enumerate}

\end{document}
